{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "context_size = 8\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(len(text))\n",
    "print(chars)\n",
    "print(vocab_size)\n",
    "\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_ix[c] for c in s]\n",
    "decode = lambda x: ''.join([ix_to_char[i] for i in x])\n",
    "decode_torch = lambda x: ''.join([ix_to_char[i.item()] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "n = int(len(data) * 0.9)\n",
    "\n",
    "# Split data into train and test (test unused for now)\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(context_size * embedding_dim, 128*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128*2, vocab_size)\n",
    "        )\n",
    "\n",
    "    def init_weights(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.kaiming_normal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a B x T tensor, where B is the batch size and T is the context size\n",
    "        B, T = x.size()\n",
    "\n",
    "        assert T == context_size\n",
    "\n",
    "        # Embed the input\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Flatten the input\n",
    "        x = x.view(B, -1)\n",
    "\n",
    "        # Pass through the MLP to get logits\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM().to(device)\n",
    "\n",
    "def sample(model):\n",
    "    with torch.no_grad():\n",
    "        prompt = text[:context_size]\n",
    "        og = prompt\n",
    "        prompt = torch.tensor(encode(prompt), dtype=torch.long).to(device)\n",
    "        for i in range(100):\n",
    "            output = model(prompt.view(1, -1))\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            prompt = torch.cat([prompt, pred]).to(device)\n",
    "            og += decode_torch(pred)\n",
    "            prompt = prompt[-context_size:]\n",
    "\n",
    "        print(og)\n",
    "\n",
    "iters = 20000\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(iters + 1):\n",
    "    idx = torch.randint(0, len(train_data) - context_size, (batch_size,)).to(device)\n",
    "    batch = torch.stack([train_data[idx:idx+context_size] for idx in idx]).to(device)\n",
    "    target = train_data[idx+context_size]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch)\n",
    "\n",
    "    # Cross entropy loss manually\n",
    "    soft_output = output.exp()\n",
    "    soft_output = soft_output / soft_output.sum(dim=1, keepdim=True)\n",
    "    loss2 = -soft_output.log().gather(1, target.view(-1, 1)).mean()\n",
    "\n",
    "    loss2.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'iter {i} loss {loss2.item()}')\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        sample(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
