{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_heads = 12\n",
    "n_emb = 768\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: adjusting the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_emb, n_emb * 3)\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "        self.bias = None\n",
    "    \n",
    "    # Multi-dimentional matrix math for parallism efficiency in multi-headed attention\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # B = batches, T = sequence length, C = embedding dimension\n",
    "        # B should be 1 during inference time?\n",
    "\n",
    "        # Create the mask for attention weights if it doesn't exist or if the size of the mask is different from the size of the input\n",
    "        if self.bias is None or self.bias.size(-1) != T:\n",
    "            self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T).to(x.device)\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "        # Single head. \n",
    "        # Intuition: allows words to gain more context from other words in the sequence\n",
    "        # Multi-head.\n",
    "        # Intuition: allows words to gain different contexts from other words in the sequence compared to single head (seeking different kinds of information)\n",
    "\n",
    "        # The linear layer is trying to convert the embedding into Q, K, V matrices\n",
    "        # Intuition:\n",
    "        # - Q is what what token is looking for\n",
    "        # - K is attributes about the token that are being looked at\n",
    "        # - V is the actual values of the token that are being looked at\n",
    "        QKV_merged = self.attn(x)\n",
    "        Q, K, V = QKV_merged.split(n_emb, dim=2) # splits back into 3 after merged matrix calcs\n",
    "\n",
    "        # Remember for efficiency, Q, K, V is really a list of Qs, Ks, Vs (one per token)\n",
    "\n",
    "        # TODO: what does C // n_heads really mean\n",
    "        Q = Q.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        K = K.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        V = V.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "\n",
    "        # 1. Intuition: a matmul between Q and K gives the relative importance of each word in the sequence to each other word\n",
    "        # (this is litearlly a dot product showing similarity between two vectors)\n",
    "        # 2. This is then scaled to prevent this from getting large (by sqrt the size of an embedding)\n",
    "        att = (Q @ K.transpose(2, 3)) * 1.0 / math.sqrt(K.size(3))\n",
    "\n",
    "        # Don't allow later tokens to influence earlier tokens\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #settings 0s to -inf\n",
    "\n",
    "        # Normalizing the attention weights\n",
    "        att = F.softmax(att, dim=-1) #convert to probabilities\n",
    "\n",
    "        # Dropout. Intuition: to prevent overfitting and to allow the model to generalize better\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # Intuition: the attention weights are multiplied by the values to get the final output  \n",
    "        # Deeper intuiton: V is what should be added to the token to make it more like the tokens its paying attention to\n",
    "        out = att @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.proj(out))\n",
    "\n",
    "        # Summary: each token asks for something (Q), gets a response (K), and then adds something (V) to itself to make it more like the responder who said K\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: the MLP is the \"feedforward\" part of the transformer and \n",
    "# it adjusts the embeddings differently than the self-attention layer does \n",
    "# by having non-linearities\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Linear layer to adjust the embeddings\n",
    "        x = self.l1(x)\n",
    "\n",
    "        # Non-linear activation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Another linear layer to adjust the embeddings\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # Dropout layer\n",
    "        # Intuition: to prevent overfitting (memorizing the training data)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block is the combination of the self-attention and the MLP\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_emb)\n",
    "        self.norm2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Intuition: the residual connection is a skip connection that allows the model to learn what to add to the embeddings.\n",
    "        # This allows the gradient to flow through the residual connection to the self-attention and MLP layers\n",
    "        x = x + self.attn(self.norm1(x)) # self-attention\n",
    "\n",
    "        # Intuition: the mlp feedforward layer is the smarts of the transformer and is where knowledge from training is stored\n",
    "        x = x + self.mlp(self.norm2(x)) # MLP\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(50257, n_emb),\n",
    "            pte = nn.Embedding(512, n_emb),\n",
    "            drop = nn.Dropout(0.0),\n",
    "            blocks = nn.ModuleList([Block() for _ in range(12)]),\n",
    "            ln_f = nn.LayerNorm(n_emb),\n",
    "        ))\n",
    "        self.head = nn.Linear(n_emb, 50257, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is the input sequence of tokens\n",
    "        # t is the length of the sequence\n",
    "        t = x.size()\n",
    "\n",
    "        # Intuition: the position embeddings are added to the token embeddings to give the model information about the position of the tokens in the sequence\n",
    "        # In this case, the position embeddings are learned by the model\n",
    "        pos = torch.arange(t).unsqueeze(0) # unsqueeze to add batch dimension\n",
    "\n",
    "        # Token embeddings + position embeddings\n",
    "        # The token embeddings are the initial embeddings that are learned by the model\n",
    "        t_emb = self.transformer.wte(x)\n",
    "        p_emb = self.transformer.pte(pos)\n",
    "\n",
    "        # Dropout (TODO: learn why useful)\n",
    "        x = self.transformer.drop(t_emb + p_emb)\n",
    "\n",
    "        # Blocks\n",
    "        # Intuition: the blocks are the combination of the self-attention and the MLP\n",
    "        # The blocks are the main part of the transformer that change the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Layer norm\n",
    "        # Intuition: layer norm helps during training by normalizing the embeddings to have a mean of 0 and a standard deviation of 1\n",
    "        # This helps the model learn better by preventing the embeddings from getting too large or too small\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Intuition: the head is the final linear layer that converts the embeddings into logits (probabilities for each token) \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create a vocabulary from the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a dictionary to convert characters to integers and vice versa\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode and decode functions\n",
    "encode = lambda s: [char_to_ix[c] for c in s]\n",
    "decode = lambda x: ''.join([ix_to_char[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
