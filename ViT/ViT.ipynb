{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_heads = 6\n",
    "n_emb = 16 * 16 * 3\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "eval_iters = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_layers = 6\n",
    "\n",
    "patch_wh = 16\n",
    "n_channel = 3\n",
    "n_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):   \n",
    "    def __init__(self, patch_size=patch_wh, n_channels=n_channel):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        embed_dim = patch_size * patch_size * n_channels\n",
    "\n",
    "        self.proj = nn.Conv2d(n_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, C, H, W  = x.size()\n",
    "        \n",
    "        # A Conv2D layer is equivalent to a linear projection layer since data in/out is same size (embed_dim)\n",
    "        # Intuition: taking every number used to represent each patch (channels * width * height) and passing it through\n",
    "        # a learned linear projection in order to embed it (same dimension as start)\n",
    "        x = self.proj(x) # (B, C, H, W) --> (B, C * P^2, H / P, W / P)\n",
    "\n",
    "        # Intuition: flatten is taking each \"patch\" and converting the 2d image into a long 1d image (concatenating every row into one long row)\n",
    "        x = x.flatten(2) # (B, C * P^2, H / P, W / P) --> (B, C * P^2, H * W / P^2)\n",
    "        x = x.transpose(1, 2) # (B, C * P^2, H * W / P^2) ---> (B, H * W / P^2, C * P^2)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: adjusting the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_emb, n_emb * 3)\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "        self.bias = None\n",
    "    \n",
    "    # Multi-dimentional matrix math for parallism efficiency in multi-headed attention\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # B = batches, T = sequence length, C = embedding dimension\n",
    "        # B should be 1 during inference time?\n",
    "\n",
    "        # Create the mask for attention weights if it doesn't exist or if the size of the mask is different from the size of the input\n",
    "        # if self.bias is None or self.bias.size(-1) != T:\n",
    "            # self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T).to(x.device)\n",
    "            # self.bias.requires_grad = False\n",
    "\n",
    "        # Single head. \n",
    "        # Intuition: allows words to gain more context from other words in the sequence\n",
    "        # Multi-head.\n",
    "        # Intuition: allows words to gain different contexts from other words in the sequence compared to single head (seeking different kinds of information)\n",
    "\n",
    "        # The linear layer is trying to convert the embedding into Q, K, V matrices\n",
    "        # Intuition:\n",
    "        # - Q is what what token is looking for\n",
    "        # - K is attributes about the token that are being looked at\n",
    "        # - V is the actual values of the token that are being looked at\n",
    "        QKV_merged = self.attn(x)\n",
    "        Q, K, V = QKV_merged.split(n_emb, dim=2) # splits back into 3 after merged matrix calcs\n",
    "\n",
    "        # Remember for efficiency, Q, K, V is really a list of Qs, Ks, Vs (one per token)\n",
    "\n",
    "        # TODO: what does C // n_heads really mean\n",
    "        Q = Q.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        K = K.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        V = V.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "\n",
    "        # 1. Intuition: a matmul between Q and K gives the relative importance of each word in the sequence to each other word\n",
    "        # (this is litearlly a dot product showing similarity between two vectors)\n",
    "        # 2. This is then scaled to prevent this from getting large (by sqrt the size of an embedding)\n",
    "        att = (Q @ K.transpose(2, 3)) * 1.0 / math.sqrt(K.size(3))\n",
    "\n",
    "        # Don't allow later tokens to influence earlier tokens [NOT NEEDED IN ENCODER - VIT]\n",
    "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #settings 0s to -inf\n",
    "\n",
    "        # Normalizing the attention weights\n",
    "        att = F.softmax(att, dim=-1) #convert to probabilities\n",
    "\n",
    "        # Dropout. Intuition: to prevent overfitting and to allow the model to generalize better\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # Intuition: the attention weights are multiplied by the values to get the final output  \n",
    "        # Deeper intuiton: V is what should be added to the token to make it more like the tokens its paying attention to\n",
    "        out = att @ V\n",
    "\n",
    "        # TODO: why contiguous\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.proj(out))\n",
    "\n",
    "        # Summary: each token asks for something (Q), gets a response (K), and then adds something (V) to itself to make it more like the responder who said K\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: the MLP is the \"feedforward\" part of the transformer and \n",
    "# it adjusts the embeddings differently than the self-attention layer does \n",
    "# by having non-linearities\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear layer to adjust the embeddings\n",
    "        x = self.l1(x)\n",
    "\n",
    "        # Non-linear activation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Another linear layer to adjust the embeddings\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # Dropout layer\n",
    "        # Intuition: to prevent overfitting (memorizing the training data)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block is the combination of the self-attention and the MLP\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_emb)\n",
    "        self.norm2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Intuition: the residual connection is a skip connection that allows the model to learn what to add to the embeddings.\n",
    "        # This allows the gradient to flow through the residual connection to the self-attention and MLP layers\n",
    "        x = x + self.attn(self.norm1(x)) # self-attention\n",
    "\n",
    "        # Intuition: the mlp feedforward layer is the smarts of the transformer and is where knowledge from training is stored\n",
    "        x = x + self.mlp(self.norm2(x)) # MLP\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            patch_emb = PatchEmbedding(),\n",
    "            pte = nn.Embedding(block_size, n_emb),\n",
    "            drop = nn.Dropout(0.0),\n",
    "            blocks = nn.ModuleList([Block() for _ in range(n_layers)]),\n",
    "            ln_f = nn.LayerNorm(n_emb),\n",
    "        ))\n",
    "        self.head = nn.Linear(n_emb, n_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence of tokens\n",
    "        # t is the length of the sequence\n",
    "        t = x.size(1)\n",
    "\n",
    "        # Intuition: the position embeddings are added to the token embeddings to give the model information about the position of the tokens in the sequence\n",
    "        # In this case, the position embeddings are learned by the model\n",
    "        pos = torch.arange(t, device=device).unsqueeze(0) # unsqueeze to add batch dimension\n",
    "\n",
    "        # Patch embeddings + position embeddings\n",
    "        # The patch embeddings are the initial embeddings that are learned by the model\n",
    "        patch_emb = self.transformer.patch_emb(x)\n",
    "        p_emb = self.transformer.pte(pos)\n",
    "\n",
    "        # Dropout: to prevent overfitting (memorizing the training data)\n",
    "        x = self.transformer.drop(patch_emb + p_emb)\n",
    "\n",
    "        # Blocks\n",
    "        # Intuition: the blocks are the combination of the self-attention and the MLP\n",
    "        # The blocks are the main part of the transformer that change the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Layer norm\n",
    "        # Intuition: layer norm helps during training by normalizing the embeddings to have a mean of 0 and a standard deviation of 1\n",
    "        # This helps the model learn better by preventing the embeddings from getting too large or too small\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Intuition: the head is the final linear layer that converts the embeddings into logits (probabilities for each token) \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = GPT().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 43328256\n"
     ]
    }
   ],
   "source": [
    "# print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total Parameters: {total_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
