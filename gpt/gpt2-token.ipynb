{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file\n",
    "import json\n",
    "\n",
    "vocab = {}\n",
    "token_to_string = {}\n",
    "\n",
    "with open('vocab_wiki.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "for i in vocab:\n",
    "    token_to_string[vocab[i]] = i\n",
    "\n",
    "\n",
    "print('vocab size: ', len(vocab))\n",
    "\n",
    "def tokenize(text):\n",
    "    encoded = []\n",
    "    tokens_raw = []\n",
    "    cur = []\n",
    "    for c in tqdm.tqdm(text):\n",
    "        cur.append(c)\n",
    "        if vocab.get(\"\".join(cur)) is None:\n",
    "            encoded.append(vocab.get(\"\".join(cur[:-1])))\n",
    "            tokens_raw.append(\"\".join(cur[:-1]))\n",
    "            cur = [c]\n",
    "\n",
    "    print('chars in text: ', len(text))\n",
    "    print('tokens after encoding: ', len(tokens_raw))\n",
    "\n",
    "    return encoded, tokens_raw\n",
    "\n",
    "def decode(encoded):\n",
    "    decoded = []\n",
    "    for e in encoded:\n",
    "        decoded.append(token_to_string[e])\n",
    "    string = \"\".join(decoded)\n",
    "    # return decoded\n",
    "    return string\n",
    "\n",
    "encoded, tokens_raw = tokenize(text)\n",
    "\n",
    "print(encoded[:100])\n",
    "\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(decoded[:100])\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "data = torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "print(data[:100])\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_heads = 8\n",
    "n_emb = 512\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "eval_iters = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_layers = 8\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: adjusting the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_emb, n_emb * 3)\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "        self.bias = None\n",
    "    \n",
    "    # Multi-dimentional matrix math for parallism efficiency in multi-headed attention\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # B = batches, T = sequence length, C = embedding dimension\n",
    "        # B should be 1 during inference time?\n",
    "\n",
    "        # Create the mask for attention weights if it doesn't exist or if the size of the mask is different from the size of the input\n",
    "        if self.bias is None or self.bias.size(-1) != T:\n",
    "            self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T).to(x.device)\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "        # Single head. \n",
    "        # Intuition: allows words to gain more context from other words in the sequence\n",
    "        # Multi-head.\n",
    "        # Intuition: allows words to gain different contexts from other words in the sequence compared to single head (seeking different kinds of information)\n",
    "\n",
    "        # The linear layer is trying to convert the embedding into Q, K, V matrices\n",
    "        # Intuition:\n",
    "        # - Q is what what token is looking for\n",
    "        # - K is attributes about the token that are being looked at\n",
    "        # - V is the actual values of the token that are being looked at\n",
    "        QKV_merged = self.attn(x)\n",
    "        Q, K, V = QKV_merged.split(n_emb, dim=2) # splits back into 3 after merged matrix calcs\n",
    "\n",
    "        # Remember for efficiency, Q, K, V is really a list of Qs, Ks, Vs (one per token)\n",
    "\n",
    "        # TODO: what does C // n_heads really mean\n",
    "        Q = Q.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        K = K.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        V = V.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "\n",
    "        # 1. Intuition: a matmul between Q and K gives the relative importance of each word in the sequence to each other word\n",
    "        # (this is litearlly a dot product showing similarity between two vectors)\n",
    "        # 2. This is then scaled to prevent this from getting large (by sqrt the size of an embedding)\n",
    "        att = (Q @ K.transpose(2, 3)) * 1.0 / math.sqrt(K.size(3))\n",
    "\n",
    "        # Don't allow later tokens to influence earlier tokens\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #settings 0s to -inf\n",
    "\n",
    "        # Normalizing the attention weights\n",
    "        att = F.softmax(att, dim=-1) #convert to probabilities\n",
    "\n",
    "        # Dropout. Intuition: to prevent overfitting and to allow the model to generalize better\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # Intuition: the attention weights are multiplied by the values to get the final output  \n",
    "        # Deeper intuiton: V is what should be added to the token to make it more like the tokens its paying attention to\n",
    "        out = att @ V\n",
    "\n",
    "        # TODO: why contiguous\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.proj(out))\n",
    "\n",
    "        # Summary: each token asks for something (Q), gets a response (K), and then adds something (V) to itself to make it more like the responder who said K\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: the MLP is the \"feedforward\" part of the transformer and \n",
    "# it adjusts the embeddings differently than the self-attention layer does \n",
    "# by having non-linearities\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear layer to adjust the embeddings\n",
    "        x = self.l1(x)\n",
    "\n",
    "        # Non-linear activation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Another linear layer to adjust the embeddings\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # Dropout layer\n",
    "        # Intuition: to prevent overfitting (memorizing the training data)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block is the combination of the self-attention and the MLP\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_emb)\n",
    "        self.norm2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Intuition: the residual connection is a skip connection that allows the model to learn what to add to the embeddings.\n",
    "        # This allows the gradient to flow through the residual connection to the self-attention and MLP layers\n",
    "        x = x + self.attn(self.norm1(x)) # self-attention\n",
    "\n",
    "        # Intuition: the mlp feedforward layer is the smarts of the transformer and is where knowledge from training is stored\n",
    "        x = x + self.mlp(self.norm2(x)) # MLP\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, n_emb),\n",
    "            pte = nn.Embedding(block_size, n_emb),\n",
    "            drop = nn.Dropout(0.0),\n",
    "            blocks = nn.ModuleList([Block() for _ in range(n_layers)]),\n",
    "            ln_f = nn.LayerNorm(n_emb),\n",
    "        ))\n",
    "        self.head = nn.Linear(n_emb, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence of tokens\n",
    "        # t is the length of the sequence\n",
    "        t = x.size(1)\n",
    "\n",
    "        # Intuition: the position embeddings are added to the token embeddings to give the model information about the position of the tokens in the sequence\n",
    "        # In this case, the position embeddings are learned by the model\n",
    "        pos = torch.arange(t, device=device).unsqueeze(0) # unsqueeze to add batch dimension\n",
    "\n",
    "        # Token embeddings + position embeddings\n",
    "        # The token embeddings are the initial embeddings that are learned by the model\n",
    "        t_emb = self.transformer.wte(x)\n",
    "        p_emb = self.transformer.pte(pos)\n",
    "\n",
    "        # Dropout: to prevent overfitting (memorizing the training data)\n",
    "        x = self.transformer.drop(t_emb + p_emb)\n",
    "\n",
    "        # Blocks\n",
    "        # Intuition: the blocks are the combination of the self-attention and the MLP\n",
    "        # The blocks are the main part of the transformer that change the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Layer norm\n",
    "        # Intuition: layer norm helps during training by normalizing the embeddings to have a mean of 0 and a standard deviation of 1\n",
    "        # This helps the model learn better by preventing the embeddings from getting too large or too small\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Intuition: the head is the final linear layer that converts the embeddings into logits (probabilities for each token) \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly select a batch of data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Intuition: the block size is the length of the sequence that the model is trying to predict\n",
    "    # X is the input sequence of tokens, Y is the target sequence of tokens\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = GPT().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in tqdm.tqdm(range(eval_iters)):\n",
    "\n",
    "            # Sample a batch of data and forward pass\n",
    "            X, Y = get_batch(split)\n",
    "            # print(X.shape, Y.shape)\n",
    "            logits = model(X)\n",
    "\n",
    "            # Compute loss \n",
    "            # Intuition: cross entropy will convert logits into probabilities and compute the loss between the probabilities and the targets\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), Y.view(B * T))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iter in tqdm.tqdm(range(max_iters)):\n",
    "    if iter % eval_interval == 0:\n",
    "        print(\"Estimate\")\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Validation Loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(xb)\n",
    "    B, T, C = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B * T, C), yb.view(B * T))\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients (this ensures training stability for large models)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"gpt_model.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total Parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_text, max_new_tokens=1000):\n",
    "    model.eval()\n",
    "    generated = torch.tensor(tokenize(start_text)[0], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    for _ in tqdm.trange(max_new_tokens):\n",
    "\n",
    "        # Take the last block_size tokens from the generated text and forward pass\n",
    "        logits = model(generated[:, -block_size:])\n",
    "\n",
    "        # The shape of logits is (batch_size, block_size, vocab_size)\n",
    "        # so we need to convert it to (batch_size, vocab_size) to get the next token\n",
    "        # -1 takes the prediction for the last token\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Decode the generated text\n",
    "    return decode(generated.squeeze().tolist())\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load(\"gpt_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Example usage\n",
    "start_text = \"   \"\n",
    "print(generate(model, start_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
