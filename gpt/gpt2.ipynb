{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paul_graham_essays.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create a vocabulary from the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a dictionary to convert characters to integers and vice versa\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode and decode functions\n",
    "encode = lambda s: [char_to_ix[c] for c in s]\n",
    "decode = lambda x: ''.join([ix_to_char[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_heads = 6\n",
    "n_emb = 384\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "eval_iters = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: adjusting the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(n_emb, n_emb * 3)\n",
    "        self.attn_dropout = nn.Dropout(0.0)\n",
    "        self.resid_dropout = nn.Dropout(0.0)\n",
    "        self.proj = nn.Linear(n_emb, n_emb)\n",
    "\n",
    "        self.bias = None\n",
    "    \n",
    "    # Multi-dimentional matrix math for parallism efficiency in multi-headed attention\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.size() # B = batches, T = sequence length, C = embedding dimension\n",
    "        # B should be 1 during inference time?\n",
    "\n",
    "        # Create the mask for attention weights if it doesn't exist or if the size of the mask is different from the size of the input\n",
    "        if self.bias is None or self.bias.size(-1) != T:\n",
    "            self.bias = torch.tril(torch.ones(T, T)).view(1, 1, T, T).to(x.device)\n",
    "            self.bias.requires_grad = False\n",
    "\n",
    "        # Single head. \n",
    "        # Intuition: allows words to gain more context from other words in the sequence\n",
    "        # Multi-head.\n",
    "        # Intuition: allows words to gain different contexts from other words in the sequence compared to single head (seeking different kinds of information)\n",
    "\n",
    "        # The linear layer is trying to convert the embedding into Q, K, V matrices\n",
    "        # Intuition:\n",
    "        # - Q is what what token is looking for\n",
    "        # - K is attributes about the token that are being looked at\n",
    "        # - V is the actual values of the token that are being looked at\n",
    "        QKV_merged = self.attn(x)\n",
    "        Q, K, V = QKV_merged.split(n_emb, dim=2) # splits back into 3 after merged matrix calcs\n",
    "\n",
    "        # Remember for efficiency, Q, K, V is really a list of Qs, Ks, Vs (one per token)\n",
    "\n",
    "        # TODO: what does C // n_heads really mean\n",
    "        Q = Q.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        K = K.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "        V = V.view(B, T, n_heads, C // n_heads).transpose(1, 2)\n",
    "\n",
    "        # 1. Intuition: a matmul between Q and K gives the relative importance of each word in the sequence to each other word\n",
    "        # (this is litearlly a dot product showing similarity between two vectors)\n",
    "        # 2. This is then scaled to prevent this from getting large (by sqrt the size of an embedding)\n",
    "        att = (Q @ K.transpose(2, 3)) * 1.0 / math.sqrt(K.size(3))\n",
    "\n",
    "        # Don't allow later tokens to influence earlier tokens\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) #settings 0s to -inf\n",
    "\n",
    "        # Normalizing the attention weights\n",
    "        att = F.softmax(att, dim=-1) #convert to probabilities\n",
    "\n",
    "        # Dropout. Intuition: to prevent overfitting and to allow the model to generalize better\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # Intuition: the attention weights are multiplied by the values to get the final output  \n",
    "        # Deeper intuiton: V is what should be added to the token to make it more like the tokens its paying attention to\n",
    "        out = att @ V\n",
    "\n",
    "        # TODO: why contiguous\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.proj(out))\n",
    "\n",
    "        # Summary: each token asks for something (Q), gets a response (K), and then adds something (V) to itself to make it more like the responder who said K\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: the MLP is the \"feedforward\" part of the transformer and \n",
    "# it adjusts the embeddings differently than the self-attention layer does \n",
    "# by having non-linearities\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear layer to adjust the embeddings\n",
    "        x = self.l1(x)\n",
    "\n",
    "        # Non-linear activation\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Another linear layer to adjust the embeddings\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # Dropout layer\n",
    "        # Intuition: to prevent overfitting (memorizing the training data)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The block is the combination of the self-attention and the MLP\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttention()\n",
    "        self.mlp = MLP()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n_emb)\n",
    "        self.norm2 = nn.LayerNorm(n_emb)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Intuition: the residual connection is a skip connection that allows the model to learn what to add to the embeddings.\n",
    "        # This allows the gradient to flow through the residual connection to the self-attention and MLP layers\n",
    "        x = x + self.attn(self.norm1(x)) # self-attention\n",
    "\n",
    "        # Intuition: the mlp feedforward layer is the smarts of the transformer and is where knowledge from training is stored\n",
    "        x = x + self.mlp(self.norm2(x)) # MLP\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(vocab_size, n_emb),\n",
    "            pte = nn.Embedding(block_size, n_emb),\n",
    "            drop = nn.Dropout(0.0),\n",
    "            blocks = nn.ModuleList([Block() for _ in range(n_layers)]),\n",
    "            ln_f = nn.LayerNorm(n_emb),\n",
    "        ))\n",
    "        self.head = nn.Linear(n_emb, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence of tokens\n",
    "        # t is the length of the sequence\n",
    "        t = x.size(1)\n",
    "\n",
    "        # Intuition: the position embeddings are added to the token embeddings to give the model information about the position of the tokens in the sequence\n",
    "        # In this case, the position embeddings are learned by the model\n",
    "        pos = torch.arange(t, device=device).unsqueeze(0) # unsqueeze to add batch dimension\n",
    "\n",
    "        # Token embeddings + position embeddings\n",
    "        # The token embeddings are the initial embeddings that are learned by the model\n",
    "        t_emb = self.transformer.wte(x)\n",
    "        p_emb = self.transformer.pte(pos)\n",
    "\n",
    "        # Dropout: to prevent overfitting (memorizing the training data)\n",
    "        x = self.transformer.drop(t_emb + p_emb)\n",
    "\n",
    "        # Blocks\n",
    "        # Intuition: the blocks are the combination of the self-attention and the MLP\n",
    "        # The blocks are the main part of the transformer that change the embeddings to contain more rich contextual information influenced by the other words in the sequence\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Layer norm\n",
    "        # Intuition: layer norm helps during training by normalizing the embeddings to have a mean of 0 and a standard deviation of 1\n",
    "        # This helps the model learn better by preventing the embeddings from getting too large or too small\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Intuition: the head is the final linear layer that converts the embeddings into logits (probabilities for each token) \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly select a batch of data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Intuition: the block size is the length of the sequence that the model is trying to predict\n",
    "    # X is the input sequence of tokens, Y is the target sequence of tokens\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = GPT().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "\n",
    "            # Sample a batch of data and forward pass\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)\n",
    "\n",
    "            # Compute loss \n",
    "            # Intuition: cross entropy will convert logits into probabilities and compute the loss between the probabilities and the targets\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, C), Y.view(B * T))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iter in tqdm.tqdm(range(max_iters)):\n",
    "    if iter % eval_interval == 0:\n",
    "        print(\"Estimate\")\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Validation Loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(xb)\n",
    "    B, T, C = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B * T, C), yb.view(B * T))\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients (this ensures training stability for large models)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"gpt_model.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 10827264\n"
     ]
    }
   ],
   "source": [
    "# print total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total Parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/3czpnnw14g5bghmx3ykf35yc0000gp/T/ipykernel_67882/2458595721.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"gpt_model_paul_graham.pth\", map_location=device))\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to a great startup is that there are pretty much where you're supposed, but it couldn't sign parallelizable your willfulness make you run in it something.It oretgan the meantime as if you're suited a startup operating on a hysical growth, you have to create a very weaknesse writing about it in redesign. Public school essays are you have at least one at  ramen private calls about what it's worthwhich. There are good at some problems yet in most impressive to look the user.The other thing most startups are people so much for the other people who did.  I'm just saying no one that made up with certain a program in was office, whether you might do whatever they want, or deciding your computers will.The attitude to ancuse you that the economy means you should never write software entricting. I never hope to risk, so I use this quality I imagined reading companies have been stopped by you, though in general phone at a mercy gradually computer fashion to use the track is for something for reasons; imitations you h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_text, max_new_tokens=1000):\n",
    "    model.eval()\n",
    "    generated = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    for _ in tqdm.trange(max_new_tokens):\n",
    "        \n",
    "        # Take the last block_size tokens from the generated text and forward pass\n",
    "        logits = model(generated[:, -block_size:])\n",
    "\n",
    "        # The shape of logits is (batch_size, block_size, vocab_size) \n",
    "        # so we need to convert it to (batch_size, vocab_size) to get the next token\n",
    "        # -1 takes the prediction for the last token\n",
    "        logits = logits[:, -1, :] \n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "    # Decode the generated text\n",
    "    return decode(generated.squeeze().tolist())\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load(\"gpt_model_paul_graham.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Example usage\n",
    "start_text = \"The answer to a great startup is\"\n",
    "print(generate(model, start_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
